#!/bin/bash
##  sbatch launching script template                Sept 2017  MKuiper
## -A generic script to launch a series of production namd runs
#--------------------------------------------------------------- 
#         Make all changes in "master_config_file"             #
#     -you shouldn't have to change anything in here!          #
#---------------------------------------------------------------
#-- Sbatch parameters:------------------------------------------
#-- the X values will be substituted with a values from the master_config_file 
#-- when you populate job directories with ./mdwf -p 

#SBATCH --nodes=X
#SBATCH --time=X
#SBATCH --tasks-per-node=X
#SBATCH --gres gpu:X

production_script=X

module load openmpi
module load X                # module file

NNODE=$SLURM_JOB_NUM_NODES
NTASKS=$SLURM_NTASKS_PER_NODE

# calculate total processes (P) and processes per node (PPN) (for multinode jobs) 
PPN=`expr $SLURM_NTASK_PER_NODE - 1`
P="$(($PPN * $SLURM_NNODES))"

# GPUS=" +idlepoll +devices $CUDA_VISIBLE_DEVICES"  <- this seems to cause troulbe with GPU allocation
GPUS=" +idlepoll " 

# For trouble shooting:
echo "cuda devices: production" > cuda.info
echo $CUDA_VISIBLE_DEVICES >> cuda.info
echo $GPUS >> cuda.info
nvidia-smi >> cuda.info

# Update current configuration files from above directory. 
# cp ../*.py . 
# cp ../namd* . 

scontrol show job $SLURM_JOBID >> scontrol.info
#--------------------------------------------------------------------------------------------
# Python script for preprocessing data
if [ -f ../prejob_processing.py ]; 
 then python3 ../prejob_processing.py $SLURM_JOBID -production;
 else echo "No processing script" > pausejob
fi 

# Prevent job from running if pausejob is present
if [ -f pausejob ]; then 
  scontrol show job $SLURM_JOBID >> last_slurm.info;
  exit; 
fi

# Run production job:
if [ $NNODE == 1 ]; then

 namd2 +ppn $NTASKS $GPUS $production_script > temp_working_outputfile.out 2> temp_working_errorsfile.err;

fi 

#--------------------------------------------------------------------------------------------
# Record average timing numbers from namd output (from days/ns converted to ns/day)  
#nsday=`less temp_working_outputfile.out |grep CPUs | tail -n 3 | awk '{ sum += $8; } END { if (NR > 0) print NR/sum; } '` 
nsday=`less temp_working_outputfile.out |grep TIMING | tail -n 3 | awk '{ sum += substr($5,1,6); } END { if (NR > 0) print 0.1728/(sum/NR); } '` 
hours=`less temp_working_outputfile.out |grep CPUTime | awk '{ print $2/3600; } '` 
ddate=`date +%d/%b/%Y`
echo $ddate " JobId:" $SLURM_JOBID "  NodeList:" $SLURM_JOB_NODELIST " cpus:" $NTASKS " GpuList:" $CUDA_VISIBLE_DEVICES " ns/day:" $nsday " Total hours:" $hours >> timing.info

# Python script for postprocessing data
if [ -f ../postjob_processing.py ]; 
  then python3 ../postjob_processing.py $SLURM_JOBID -production;
  else echo "No processing script" > pausejob;
fi

# Prevent job from running if pausejob is present. Also to troubleshoot crashing jobs. 
if [ -f pausejob ]; then
  scontrol show job $SLURM_JOBID >> last_slurm.info;
  exit;
fi

# Launch next job: 
sbatch sbatch_production_script 

