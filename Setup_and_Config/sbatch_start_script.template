#!/bin/bash
##  sbatch launching script template                Sept  2017  MKuiper
## -A generic script to launch a series of production namd runs
## on the Avoca bluegene cluster at vlsci. 
#--------------------------------------------------------------- 
#         Make all changes in "master_config_file"             #
#     -you shouldn't have to change anything in here!          #
#---------------------------------------------------------------
#-- Sbatch parameters:------------------------------------------
#-- the X values will be substituted with a values 
#-- from the master_config_file when you populate 
#-- job directories with:  ./mdwf -p 

#SBATCH --nodes=X
#SBATCH --time=X
#SBATCH --tasks-per-node=X
#SBATCH --gres gpu:X

optimize_script=X      

module load openmpi
module load X			# namd module file

NNODE=$SLURM_JOB_NUM_NODES
NTASKS=$SLURM_NTASKS_PER_NODE

# calculate total processes (P) and processes per node (PPN)(for multiple node jobs) 
# PPN=`expr $SLURM_NTASKS_PER_NODE -1`
# P="$(($PPN * $SLURM_NODES))"

GPUS=" +idlepoll "

echo "cuda devices" > cuda.info
echo $CUDA_VISIBLE_DEVICES >> cuda.info
echo $GPUS >> cuda.info
nvidia-smi >> cuda.info

scontrol show job $SLURM_JOBID > scontrol.info

# update current working scripts from above directory
# cp ../*.py . 
# cp ../namd* . 

#--------------------------------------------------------------------------------
## python script to initiate job and setup variables.-----------
## pause if preprocessing script is not present. 

if [ -f ../prejob_processing.py ];
 then 
      python3 ../prejob_processing.py $SLURM_JOBID -optimize;
 else 
      echo "No processing script" > pausejob;
fi

## prevent job from running if pausejob is present
if [ -f pausejob ]; then exit; fi

# submit job to the cluster:------------------------------------
if [ $NNODE == 1 ]; then

 namd2 +ppn $NTASKS $GPUS $optimize_script > temp_working_outputfile.out 2> temp_working_errorsfile.err;

fi
#--------------------------------------------------------------------------------
# record average timing numbers from namd output (from days/ns converted to ns/day)
 nsday=`less temp_working_outputfile.out |grep CPUs | tail -n 3 | awk '{ sum += $8; } END { if (NR > 0) print NR/sum; } '`
 ddate=`date +%d/%b/%Y`
 echo $ddate " JobId:" $SLURM_JOBID "  NodeList:" $SLURM_JOB_NODELIST " cpus:" $NTASKS " GpuList:" $CUDA_VISIBLE_DEVICES " ns/day:" $nsday >> timing.info

## python script to cleanup and redirect data-------------------- 
if [ -f ../postjob_processing.py ];
 then 
      python3 ../postjob_processing.py $SLURM_JOBID -optimize;
 else 
      echo "No processing script" > pausejob;
fi

## prevent job from running if pausejob is present
if [ -f pausejob ]; then exit; fi

# launch next job segment:------------------------------------------------------

 sbatch sbatch_production_script

#--------------------------------------------------------------------------------
